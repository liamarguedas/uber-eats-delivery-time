{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c8a8eb3b-556c-4305-89b3-76ad58207f21",
      "metadata": {
        "id": "c8a8eb3b-556c-4305-89b3-76ad58207f21"
      },
      "source": [
        "# Model, predict and solve the problem.\n",
        "\n",
        "Modeling data before training a machine learning (ML) model is crucial for various reasons. It involves understanding the data's characteristics, which aids in identifying potential issues and assessing its suitability for the ML task. Additionally, data modeling enables preprocessing steps like handling missing values, outlier detection, and feature scaling, which enhance data quality. Through feature engineering, new features can be created or existing ones transformed to improve the model's performance. Dimensionality reduction techniques reduce the number of features, improving efficiency and preventing overfitting. Modeling data facilitates model selection based on data characteristics and requirements. Finally, data modeling enables evaluation and validation, ensuring the model's reliability and generalization capabilities before deployment. Overall, data modeling is fundamental for optimizing data, improving model performance, and facilitating informed decision-making in ML."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51bf3ece-0a95-421e-8ebf-5f5967044021",
      "metadata": {
        "id": "51bf3ece-0a95-421e-8ebf-5f5967044021"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1576f8d1-9325-496e-8f47-8bcd57534753",
      "metadata": {
        "id": "1576f8d1-9325-496e-8f47-8bcd57534753"
      },
      "outputs": [],
      "source": [
        "# Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "sns.set_style('whitegrid')\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fb1a34f-b398-4667-b640-1c935bcb1197",
      "metadata": {
        "id": "7fb1a34f-b398-4667-b640-1c935bcb1197"
      },
      "source": [
        "## Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c8eea28-8c4e-4e47-91ac-f30c7bd67361",
      "metadata": {
        "id": "8c8eea28-8c4e-4e47-91ac-f30c7bd67361"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/modeling-data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b91fc23-c831-413d-91c0-d7faef268e67",
      "metadata": {
        "tags": [],
        "id": "2b91fc23-c831-413d-91c0-d7faef268e67"
      },
      "source": [
        "## Feature Engineering\n",
        "Feature engineering is a fundamental process of transforming raw data to make it more suitable for machine learning algorithms. Its purpose is to select, create, and modify features (input variables) to enhance the performance of a machine learning model.\n",
        "\n",
        "### Cardinality\n",
        "The values of a categorical variable are selected from a group of categories, also called labels. For example, in the variable gender the categories are male and female, whereas in the variable city the labels could be London, Manchester, Brighton, and so on.\n",
        "\n",
        "Categorical variables can contain different numbers of categories. The variable \"gender\" contains only 2 labels, but a variable like \"city\" or \"postcode\" can contain a huge number of labels.\n",
        "\n",
        "The number of different labels is known as cardinality. A high number of labels within a variable is known as high cardinality. High cardinality poses the following challenges:\n",
        "\n",
        "- Variables with too many labels tend to dominate those with only a few labels, particularly in decision tree-based algorithms.\n",
        "- High cardinality may introduce noise.\n",
        "- Some of the labels may only be present in the training data set and not in the test set, so machine learning algorithms may over-fit to the training set.\n",
        "- Some labels may appear only in the test set, leaving the machine learning algorithms unable to perform a calculation over the new (unseen) observation.\n",
        "\n",
        "let's check cardinality of the Categorical Features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37033196-8ac8-45b4-9ea2-5beca3e5e42f",
      "metadata": {
        "id": "37033196-8ac8-45b4-9ea2-5beca3e5e42f"
      },
      "outputs": [],
      "source": [
        "for feature in data.select_dtypes(include = 'O').columns:\n",
        "\n",
        "        print(f'{feature}: {data[feature].nunique()}')\n",
        "\n",
        "print()\n",
        "print(f\"data len: {data.shape[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18d6fa90-7ebb-4415-90af-94c118995fd9",
      "metadata": {
        "id": "18d6fa90-7ebb-4415-90af-94c118995fd9"
      },
      "source": [
        "### Rare Labels\n",
        "\n",
        "Categorical variables are those whose values are selected from a group of categories, also called labels. Labels may have different frequencies. Some categories appear a lot in the dataset, some others appear only in a small number of observations.\n",
        "\n",
        "For example, in a dataset with information about loan applicants where one of the variables is \"city\" where the applicant lives, cities like \"New York\" may appear a lot in the data because New York has a huge population, whereas smaller towns like \"Leavenworth\" will appear only on a few occasions (population ~2000 people), because the population there is very small. A borrower is more likely to live in New York than in any other city because it has a bigger population.\n",
        "\n",
        "More specifically,\n",
        "\n",
        "- Rare categories can cause over-fitting, particularly in tree-based methods.\n",
        "\n",
        "- Infrequent labels may add noise, which could cause over-fitting.\n",
        "\n",
        "- Rare labels may be present only in training set, therefore causing over-fitting.\n",
        "\n",
        "- Rare labels may appear only in the test set. Thus, the ML model will not know how to evaluate it.\n",
        "\n",
        "**Note:** Sometimes rare values, are indeed important. For example, if we are building a model to predict fraudulent loan applications, which are by nature rare, then a rare value in a certain variable, may be indeed very predictive. This rare value could be telling us that the observation is most likely a fraudulent application, and therefore we should not to ignore it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "259f14b8-9411-4de0-b812-37ea307fcf6d",
      "metadata": {
        "id": "259f14b8-9411-4de0-b812-37ea307fcf6d"
      },
      "outputs": [],
      "source": [
        "fig, ((ax1, ax2, ax3, ax4), (ax5, ax6, ax7, ax8)) = plt.subplots(ncols = 4, nrows = 2, figsize = (10, 6))\n",
        "\n",
        "axes = [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8]\n",
        "\n",
        "# For each categorical variable\n",
        "for ax, feature in enumerate(data.select_dtypes(include = 'O').columns):\n",
        "\n",
        "    pd.Series(data[feature].value_counts() / len(data)).sort_values(ascending = False).plot(kind = 'bar', ax = axes[ax])\n",
        "    axes[ax].set_title(feature)\n",
        "    axes[ax].axhline(y = 0.05, color = 'red')\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66685efb-a28d-4c67-8b8a-9e83c74bfa33",
      "metadata": {
        "id": "66685efb-a28d-4c67-8b8a-9e83c74bfa33"
      },
      "source": [
        "**Festival**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a50fb2b-b399-4578-a95c-94dc02e568d5",
      "metadata": {
        "id": "2a50fb2b-b399-4578-a95c-94dc02e568d5"
      },
      "outputs": [],
      "source": [
        "data = data.drop('Festival', axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31528aad-5206-4526-9d18-898ad1099f12",
      "metadata": {
        "id": "31528aad-5206-4526-9d18-898ad1099f12"
      },
      "source": [
        "**City**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a410f9c8-fc77-4050-bba7-5f50e57957ee",
      "metadata": {
        "id": "a410f9c8-fc77-4050-bba7-5f50e57957ee"
      },
      "outputs": [],
      "source": [
        "data['City'] = data['City'].apply(lambda value: 'Urban' if value == 'Semi-Urban' else value)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1349ec7e-10e2-4dd8-b45c-c7007a53de4c",
      "metadata": {
        "id": "1349ec7e-10e2-4dd8-b45c-c7007a53de4c"
      },
      "source": [
        "### Data transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bc30e26-5651-4f4f-9bf9-3a8c34ee67b0",
      "metadata": {
        "id": "0bc30e26-5651-4f4f-9bf9-3a8c34ee67b0"
      },
      "outputs": [],
      "source": [
        "import scipy.stats as stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53d97b7f-ce87-49cb-8938-4736d3933495",
      "metadata": {
        "id": "53d97b7f-ce87-49cb-8938-4736d3933495"
      },
      "outputs": [],
      "source": [
        "ContinuousFeatures = data.drop(['Vehicle_condition', 'multiple_deliveries',\n",
        "                               'OrderTime', 'Delivery_person_Age', 'Time_taken(min)'], axis = 1).select_dtypes(exclude = 'O').columns\n",
        "\n",
        "ContinuousFeatures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d663b787-f446-4248-a3e9-f7b1a776f789",
      "metadata": {
        "id": "d663b787-f446-4248-a3e9-f7b1a776f789"
      },
      "outputs": [],
      "source": [
        "fig, ((ax1, ax2, ax3, ax4), (ax5, ax6, ax7, ax8)) = plt.subplots(ncols = 4, nrows = 2, figsize = (12, 5), sharey = True)\n",
        "\n",
        "# Distribution with no transformations\n",
        "sns.histplot(data = data, x = ContinuousFeatures[0], ax = ax1, bins = 20)\n",
        "sns.histplot(data = data, x = ContinuousFeatures[1], ax = ax5, bins = 20)\n",
        "ax1.set_title('No transformations')\n",
        "ax1.set_xlabel('')\n",
        "ax5.set_xlabel('')\n",
        "ax1.set_ylabel(ContinuousFeatures[0])\n",
        "ax5.set_ylabel(ContinuousFeatures[1])\n",
        "\n",
        "# Yeo-Johnson transformation\n",
        "temp1, param1 = stats.yeojohnson(data[ContinuousFeatures[0]].astype('float'))\n",
        "temp2, param2 = stats.yeojohnson(data[ContinuousFeatures[1]].astype('float'))\n",
        "sns.histplot(temp1, ax = ax2, bins = 20)\n",
        "sns.histplot(temp2, ax = ax6, bins = 20)\n",
        "ax2.set_title('Yeo-Johnson transformation')\n",
        "ax2.set_xlabel('')\n",
        "ax6.set_xlabel('')\n",
        "\n",
        "# Box-Cox transformation\n",
        "temp3, param3 = stats.boxcox(data[ContinuousFeatures[0]].astype('float'))\n",
        "temp4, param4 = stats.boxcox(data[ContinuousFeatures[1]].astype('float'))\n",
        "sns.histplot(temp3, ax = ax3, bins = 20)\n",
        "sns.histplot(temp4, ax = ax7, bins = 20)\n",
        "ax3.set_title('Box-Cox transformation')\n",
        "ax3.set_xlabel('')\n",
        "ax7.set_xlabel('')\n",
        "\n",
        "# Log transformation\n",
        "sns.histplot(np.log(data[ContinuousFeatures[0]]), ax = ax4, bins = 20)\n",
        "sns.histplot(np.log(data[ContinuousFeatures[0]]), ax = ax8, bins = 20)\n",
        "ax3.set_title('Log transformation')\n",
        "ax3.set_xlabel('')\n",
        "ax7.set_xlabel('');"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaef2fb7-c465-4ede-ac32-34672bb9072a",
      "metadata": {
        "id": "eaef2fb7-c465-4ede-ac32-34672bb9072a"
      },
      "source": [
        "### Feature Encoding\n",
        "feature encoding is important to convert non-numeric data into a numerical representation that machine learning algorithms can process. It ensures algorithm compatibility, preserves relevant information, reduces dimensionality, handles non-numeric data, and ultimately improves model performance.\n",
        "\n",
        "There is too many Encoding techniques: OneHot, Ordinal, Frequency, Ordered, Mean, Weight of evidence, etc... It's worth noting that there is no one-size-fits-all encoding method, and the choice of encoding technique depends on the specific characteristics of the data, the machine learning algorithms being used, and the goals of the analysis. It is often helpful to experiment with different encoding approaches and evaluate their impact on the model's performance before deciding which one is most suitable for a particular classification problem.\n",
        "\n",
        "For this particular case I have decided to use: **OneHot Encoding**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0fd6a9c-246f-4bb3-a9d8-353be511ef04",
      "metadata": {
        "id": "c0fd6a9c-246f-4bb3-a9d8-353be511ef04"
      },
      "outputs": [],
      "source": [
        "data.drop('Time_taken(min)', axis = 1).columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81f17cd7-b78a-49da-95b9-66d18eb49f07",
      "metadata": {
        "id": "81f17cd7-b78a-49da-95b9-66d18eb49f07"
      },
      "outputs": [],
      "source": [
        "data = pd.get_dummies(data, drop_first = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c93c1ee8-fed5-4c05-afa4-e31815ee5c80",
      "metadata": {
        "id": "c93c1ee8-fed5-4c05-afa4-e31815ee5c80"
      },
      "source": [
        "### X, y\n",
        "\n",
        "You can think of X as the input data, and y as the corresponding output or target values that you are trying to predict or model using machine learning algorithms. The goal is to learn a function or a relationship between X and y, so that given new input data, the model can make accurate prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f04d0d53-dbe8-4816-88ed-9cf12d17d37b",
      "metadata": {
        "id": "f04d0d53-dbe8-4816-88ed-9cf12d17d37b"
      },
      "outputs": [],
      "source": [
        "# Reseting Index\n",
        "\n",
        "data.reset_index(drop = True, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e96ba35-1713-4a78-b597-3acc4e28e4c1",
      "metadata": {
        "id": "9e96ba35-1713-4a78-b597-3acc4e28e4c1"
      },
      "outputs": [],
      "source": [
        "X = data.drop('Time_taken(min)', axis = 1)\n",
        "y = data['Time_taken(min)']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44132520-86c1-49ec-819a-c325311a2422",
      "metadata": {
        "id": "44132520-86c1-49ec-819a-c325311a2422"
      },
      "source": [
        "**Feature engineering notes:**\n",
        "\n",
        "-  Cardinality in all categorical variables is controlled, being order_day the feature with more cardinality (7)\n",
        "-  City and Festival both had a label that only appeared few times (Festival: Yes, City Semi-Urban) For festival I decided to remove the entire feature from the training dataset, since deliting \"Yes\" mean that there will be only \"No\" in the variable, this means having no predicting value. For city I decided to map those Semi-Urban as Urban.\n",
        "-  There is no transformations that can be applied really satisfies the normal distribution so linear models need to be avoided.\n",
        "-  Data was encoded using OneHot encoding since this is a regression problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "249864e7-884b-4098-8524-934f7706786b",
      "metadata": {
        "id": "249864e7-884b-4098-8524-934f7706786b"
      },
      "source": [
        "## Performance & Error Metrics\n",
        "\n",
        "Using performance and error metrics is essential for evaluating machine learning models. These metrics provide a quantitative assessment of model performance, aiding in model selection and hyperparameter tuning. They help detect overfitting or underfitting and enable performance monitoring over time. Metrics also play a crucial role in decision-making processes, allowing stakeholders to assess the feasibility and effectiveness of deploying ML models in real-world settings. Performance and error metrics provide a systematic and objective approach to evaluate and optimize ML models for optimal performance and decision-making.\n",
        "\n",
        "The metrics I am gonna take into consideration are:\n",
        "\n",
        "- **Mean Squared Error (MSE):** MSE is a common metric used to measure the average squared difference between the predicted and actual values in regression problems. It quantifies the overall quality of a regression model by assessing how closely its predictions align with the true values.\n",
        "\n",
        "- **Root Mean Squared Error (RMSE):** RMSE is a commonly used evaluation metric in regression problems. It is derived from the Mean Squared Error (MSE) and provides a measure of the average magnitude of the prediction errors made by a regression model. To calculate RMSE, you take the square root of the MSE.\n",
        "- **Mean Absolute Error (MAE):** MAE provides a measure of the average magnitude of the prediction errors made by the model. Unlike MSE and RMSE, which involve squaring the errors, MAE considers the absolute differences, which makes it less sensitive to outliers or instances with larger errors. The advantage of MAE is its simplicity and ease of interpretation. It represents the average absolute deviation from the true values and is expressed in the same units as the target variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb3c698c-1eb9-4d9c-bacb-4fb9d0936758",
      "metadata": {
        "id": "bb3c698c-1eb9-4d9c-bacb-4fb9d0936758"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "050eb89a-6ce8-4d1f-8f7a-a0d60d58d441",
      "metadata": {
        "id": "050eb89a-6ce8-4d1f-8f7a-a0d60d58d441"
      },
      "outputs": [],
      "source": [
        "def GetMetrics(y, predictions):\n",
        "    return {'MSE' : mean_squared_error(y, predictions),\n",
        "            'RMSE' : np.sqrt(mean_squared_error(y, predictions)),\n",
        "            'MAE': mean_absolute_error(y, predictions)}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b62c5a7-50c0-4aef-be52-62b28487b03c",
      "metadata": {
        "id": "9b62c5a7-50c0-4aef-be52-62b28487b03c"
      },
      "source": [
        "## Model Selection\n",
        "\n",
        "For this particular problem I have chosen to use a LSTM NN. A Long Short-Term Memory (LSTM) is a type of recurrent neural network (RNN) architecture that is widely used for processing sequential and time series data. LSTMs are designed to overcome the limitations of traditional RNNs in capturing long-term dependencies and preserving information over longer sequences.\n",
        "\n",
        "LSTMs are particularly effective in tasks where there are time dependencies and contextual information matters. They have been successfully applied in various domains, including natural language processing, speech recognition, machine translation, and time series forecasting.\n",
        "\n",
        "The key feature of an LSTM is its memory cell, which allows it to selectively remember or forget information over time. It consists of several gates that control the flow of information:\n",
        "\n",
        "- **Forget Gate:** Determines which information to discard from the previous cell state.\n",
        "- **Input Gate:** Decides which new information to update and add to the cell state.\n",
        "- **Output Gate:** Filters the information from the current cell state to produce the output.\n",
        "The LSTM's ability to selectively retain or discard information through these gates enables it to capture long-term dependencies and handle vanishing or exploding gradients during training.\n",
        "\n",
        "LSTMs are trained using backpropagation through time (BPTT), an extension of backpropagation where the gradient is calculated across time steps. This allows the network to learn and update its parameters by minimizing a specific loss function, such as mean squared error (MSE) or mean absolute error (MAE), depending on the regression task.\n",
        "\n",
        "By leveraging the memory cell and gates, LSTMs can model complex sequences and make predictions based on historical context. They have become a popular choice for sequential data analysis and have significantly contributed to advancements in various fields where temporal relationships are essential."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6ad4875-ba33-422f-9cb7-26b49bfdbf57",
      "metadata": {
        "id": "b6ad4875-ba33-422f-9cb7-26b49bfdbf57"
      },
      "source": [
        "## Training keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd2df68e-2bad-4adf-b07c-9f176acd5396",
      "metadata": {
        "id": "fd2df68e-2bad-4adf-b07c-9f176acd5396"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import os\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "if device_name != '/device:GPU:0':\n",
        "\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "id": "VndEXFRQB3Kn"
      },
      "id": "VndEXFRQB3Kn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stop = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 3)"
      ],
      "metadata": {
        "id": "S-BXjcM-C9rJ"
      },
      "id": "S-BXjcM-C9rJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.10, random_state = 42)"
      ],
      "metadata": {
        "id": "X2Ywk9VodNU9"
      },
      "id": "X2Ywk9VodNU9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "go827PrF2kYX"
      },
      "id": "go827PrF2kYX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, X_test.shape"
      ],
      "metadata": {
        "id": "VkDiO6MO2tDL"
      },
      "id": "VkDiO6MO2tDL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1df01bb0-c845-453b-9144-82dc429de48e",
      "metadata": {
        "id": "1df01bb0-c845-453b-9144-82dc429de48e"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(27, activation = 'relu'))\n",
        "model.add(Dense(13, activation = 'relu'))\n",
        "model.add(Dense(6, activation = 'relu'))\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(optimizer = 'adam', loss = 'mean_squared_error')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x = X_train, y = y_train, validation_data = (X_test, y_test),\n",
        "          batch_size = 1, epochs = 100, callbacks = [early_stop])"
      ],
      "metadata": {
        "id": "8HV-qeHqdt4h"
      },
      "id": "8HV-qeHqdt4h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions\n",
        "predictions = model.predict(X_test)"
      ],
      "metadata": {
        "id": "GdhmI3S2dwzH"
      },
      "id": "GdhmI3S2dwzH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd8d0b78-52ca-4deb-b390-f28a0d70cbb2",
      "metadata": {
        "id": "cd8d0b78-52ca-4deb-b390-f28a0d70cbb2"
      },
      "outputs": [],
      "source": [
        "Summary = pd.DataFrame(GetMetrics(y = y_test, predictions = predictions), index = ['Score'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6652bf0-ace3-44c7-875a-46605e7ae727",
      "metadata": {
        "id": "c6652bf0-ace3-44c7-875a-46605e7ae727"
      },
      "outputs": [],
      "source": [
        "Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72cd7634-b7ff-4a5a-9458-71fe2314d925",
      "metadata": {
        "id": "72cd7634-b7ff-4a5a-9458-71fe2314d925"
      },
      "outputs": [],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ComparationTable = pd.DataFrame({\n",
        "    'Real Value' : y_test.values,\n",
        "    'Model Prediction' : [round(item[0]) for item in predictions],\n",
        "    'Difference' : y_test.values - [round(item[0]) for item in predictions],\n",
        "    'Difference %' : np.absolute((y_test.values - [round(item[0]) for item in predictions]) / y_test.values * 100)})\n",
        "\n",
        "ComparationTable.head()"
      ],
      "metadata": {
        "id": "EGreql4pZ10O"
      },
      "id": "EGreql4pZ10O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ComparationTable.describe()[1:]"
      ],
      "metadata": {
        "id": "s_JgSvqZ2s6v"
      },
      "id": "s_JgSvqZ2s6v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Our predictions\n",
        "plt.scatter(y_test, predictions)\n",
        "\n",
        "# Perfect predictions\n",
        "plt.plot(y_test, y_test, 'r')\n",
        "plt.title('Model Predictions vs. Perfect Predictions (Line)');"
      ],
      "metadata": {
        "id": "NbZDqA9IfBCd"
      },
      "id": "NbZDqA9IfBCd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "errors = y_test.values.reshape(4387, 1) - predictions\n",
        "sns.distplot(errors)\n",
        "plt.title('Residuals');"
      ],
      "metadata": {
        "id": "3gFSzP732Qu2"
      },
      "id": "3gFSzP732Qu2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Worst predictions\n",
        "ComparationTable.sort_values(by = 'Difference %', ascending = False).head(20)"
      ],
      "metadata": {
        "id": "7Bls7CDy2Gkc"
      },
      "id": "7Bls7CDy2Gkc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2) = plt.subplots(ncols = 2, nrows = 1, figsize = (10, 3))\n",
        "sns.histplot(data = ComparationTable, x = 'Difference %', ax = ax1)\n",
        "ax1.set_xlim(left = 0)\n",
        "ax1.set_title('Predictions Difference % to real value')\n",
        "\n",
        "sns.boxplot(data = ComparationTable, x = 'Difference %', ax = ax2)\n",
        "ax2.set_title('Predictions Difference % to real value');"
      ],
      "metadata": {
        "id": "tjyj3DyCDzDc"
      },
      "id": "tjyj3DyCDzDc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vWGllQNdEKJq"
      },
      "id": "vWGllQNdEKJq",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}